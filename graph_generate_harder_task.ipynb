{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1789c9",
   "metadata": {},
   "source": [
    "# Generate more challenging train/test splits\n",
    "\n",
    "Step1: draw N attributes (e.g., 10 used in our paper) for each sample, see how the trianing/validation/test dataset overlaps\n",
    "\n",
    "Step2: prune the training set, deleting those have overlapped G\n",
    "\n",
    "Step3: save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aceb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "from probing import *\n",
    "from utils.general import *\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "def get_args_parser():\n",
    "    # Training settings\n",
    "    # ======= Usually default settings\n",
    "    parser = argparse.ArgumentParser(description='GNN baselines on ogbgmol* data with Pytorch Geometrics')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')\n",
    "    parser.add_argument('--drop_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "    parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--batch_size_train', type=int, default=64,\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--num_workers', type=int, default=2,\n",
    "                        help='number of workers (default: 0)')\n",
    "    parser.add_argument('--dataset_name', type=str, default=\"ogbg-molhiv\",\n",
    "                        help='dataset name (default: ogbg-molhiv/moltox21/molpcba)')\n",
    "    parser.add_argument('--feature', type=str, default=\"full\",\n",
    "                        help='full feature or simple feature')\n",
    "    parser.add_argument('--bottle_type', type=str, default='std',\n",
    "                        help='bottleneck type, can be std or sem')\n",
    "    # ==== Model Structure ======\n",
    "        # ----- Backbone\n",
    "    parser.add_argument('--backbone_type', type=str, default='gcn',\n",
    "                        help='backbone type, can be gcn, gin, gcn_virtual, gin_virtual')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='dimensionality of hidden units in GNNs (default: 300)')  \n",
    "    parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5)')\n",
    "        # ---- SEM\n",
    "    parser.add_argument('--L', type=int, default=30,\n",
    "                        help='No. word in SEM')\n",
    "    parser.add_argument('--V', type=int, default=10,\n",
    "                        help='word size in SEM')\n",
    "                        \n",
    "        # ---- Head-type\n",
    "    parser.add_argument('--head_type', type=str, default='linear',\n",
    "                        help='Head type in interaction, linear or mlp')    \n",
    "    return parser\n",
    "\n",
    "def rnd_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "rnd_seed(10086)\n",
    "\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(args=[])\n",
    "#args = args.parse_args()\n",
    "args.device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "FIG_DIR = 'E:\\\\P45_disentanglement\\\\figures\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269fe9f",
   "metadata": {},
   "source": [
    "## Prepare for the probing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1902b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.dataset_name == 'ogbg-molhiv':\n",
    "    smiles_path = 'E:\\\\P4_Graph\\\\dataset\\\\ogbg_molhiv\\\\mapping\\\\mol.csv.gz'\n",
    "    args.batch_size = 4113\n",
    "    args.batch_size_train = 32901\n",
    "elif args.dataset_name == 'ogbg-molpcba':\n",
    "    args.batch_size = 24000\n",
    "    smiles_path = 'E:\\\\P4_Graph\\\\dataset\\\\ogbg_molpcba\\\\mapping\\\\mol.csv.gz'\n",
    "elif args.dataset_name =='ogbg-moltox21':\n",
    "    args.batch_size = 783\n",
    "    smiles_path = 'E:\\\\P4_Graph\\\\dataset\\\\ogbg_moltox21\\\\mapping\\\\mol.csv.gz'\n",
    "\n",
    "selected_prop = ['NumSaturatedRings', 'NumAromaticRings', 'NumAromaticCarbocycles', 'fr_aniline', \n",
    "                 'fr_ketone', 'fr_bicyclic', 'fr_methoxy', 'fr_para_hydroxylation', 'fr_pyridine', 'fr_benzene']\n",
    "\n",
    "dataset = PygGraphPropPredDataset(name = args.dataset_name)\n",
    "args.num_tasks = dataset.num_tasks\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size_train, shuffle=False, drop_last=True,\n",
    "                        num_workers=args.num_workers)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.batch_size, shuffle=False, drop_last=True,\n",
    "                        num_workers=args.num_workers)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, drop_last=True,\n",
    "                        num_workers=args.num_workers)\n",
    "\n",
    "train_smiles = pd.read_csv(smiles_path).iloc[split_idx['train']].smiles.values\n",
    "train_smiles = train_smiles.tolist()\n",
    "valid_smiles = pd.read_csv(smiles_path).iloc[split_idx['valid']].smiles.values\n",
    "valid_smiles = valid_smiles.tolist()\n",
    "test_smiles = pd.read_csv(smiles_path).iloc[split_idx['test']].smiles.values\n",
    "test_smiles = test_smiles.tolist()\n",
    "\n",
    "train_desc_names, train_properties = compute_properties(train_smiles)\n",
    "valid_desc_names, valid_properties = compute_properties(valid_smiles)\n",
    "test_desc_names, test_properties = compute_properties(test_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270810b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3333.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_fp, valid_fp, test_fp = [], [], []\n",
    "for prop in tqdm(selected_prop):\n",
    "    train_fp.append(np.array(train_properties[prop].values[:args.batch_size_train]>0,dtype=int))\n",
    "    valid_fp.append(np.array(valid_properties[prop].values[:args.batch_size]>0,dtype=int))\n",
    "    test_fp.append(np.array(test_properties[prop].values[:args.batch_size]>0,dtype=int))\n",
    "train_fp = np.array(train_fp).transpose()\n",
    "valid_fp = np.array(valid_fp).transpose()\n",
    "test_fp = np.array(test_fp).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96be8298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 32901/32901 [00:04<00:00, 6911.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----- For each sample in validation set, sweep the training set to see how many samples share similar finger print\n",
    "# -- This will generate a long list of the number of duplicates each training sample have\n",
    "index_template = np.arange(0,args.batch_size,1)\n",
    "dup_count_list, dup_index_list = [],[]\n",
    "for i in tqdm(range(args.batch_size_train)):\n",
    "    anchor = train_fp[i]\n",
    "    tmp1 = valid_fp.dot(anchor)==(anchor.sum())\n",
    "    tmp2 = valid_fp.dot(anchor)==valid_fp.sum(1)\n",
    "    dup_mask = np.logical_and(tmp1,tmp2)\n",
    "    dup_count = dup_mask.sum()\n",
    "    dup_index = index_template[dup_mask]\n",
    "    dup_count_list.append(dup_count)\n",
    "    dup_index_list.append(dup_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b095f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- The dup_count_list stores how many similar samples are in the validation set\n",
    "# --- To make the problem more challenging, we need to first argsort this list, and delete those numbers with the HIGHER overlapping FP's\n",
    "# --- After that, there might be less similar samples in the training set\n",
    "PRUNE_IDX = int(args.batch_size_train*0.95)   # How many data samples are pruned\n",
    "train_index_template = np.arange(0,args.batch_size_train,1)\n",
    "train_split_index = np.array(split_idx['train'])\n",
    "dup_count_list = np.array(dup_count_list)\n",
    "tmp_pd = pd.DataFrame(np.column_stack((train_index_template, dup_count_list,train_split_index)),\n",
    "                      index=train_index_template, columns=['index','count','split_index'])\n",
    "prune_pd = tmp_pd.sort_values(by='count',ascending=False)[PRUNE_IDX:]\n",
    "prune_pd_sort = prune_pd.sort_values(by='index',ascending=True)\n",
    "\n",
    "# ----- The index of the pruned dataset is saved in this npy file, during training, we use it to select samples\n",
    "sel_index = prune_pd_sort.iloc[:,0]\n",
    "sel_index = sel_index.values\n",
    "save_path = 'E:\\\\P4_Graph\\\\dataset\\\\' + args.dataset_name + '_hard.npy'\n",
    "np.save(save_path, sel_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bee4937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1646,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288478b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_pd_sort['count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b7b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22670406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1789c9",
   "metadata": {},
   "source": [
    "# Generate more challenging train/test splits\n",
    "\n",
    "Step1: draw N attributes (e.g., 10 used in our paper) for each sample, see how the trianing/validation/test dataset overlaps\n",
    "\n",
    "Step2: prune the training set, deleting those have overlapped G\n",
    "\n",
    "Step3: save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aceb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "from probing import *\n",
    "from utils.general import *\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "def get_args_parser():\n",
    "    # Training settings\n",
    "    # ======= Usually default settings\n",
    "    parser = argparse.ArgumentParser(description='GNN baselines on ogbgmol* data with Pytorch Geometrics')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')\n",
    "    parser.add_argument('--drop_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "    parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--batch_size_train', type=int, default=64,\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--num_workers', type=int, default=2,\n",
    "                        help='number of workers (default: 0)')\n",
    "    parser.add_argument('--dataset_name', type=str, default=\"ogbg-molpcba\",\n",
    "                        help='dataset name (default: ogbg-molhiv/moltox21/molpcba)')\n",
    "    parser.add_argument('--feature', type=str, default=\"full\",\n",
    "                        help='full feature or simple feature')\n",
    "    parser.add_argument('--bottle_type', type=str, default='std',\n",
    "                        help='bottleneck type, can be std or sem')\n",
    "    # ==== Model Structure ======\n",
    "        # ----- Backbone\n",
    "    parser.add_argument('--backbone_type', type=str, default='gcn',\n",
    "                        help='backbone type, can be gcn, gin, gcn_virtual, gin_virtual')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='dimensionality of hidden units in GNNs (default: 300)')  \n",
    "    parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5)')\n",
    "        # ---- SEM\n",
    "    parser.add_argument('--L', type=int, default=30,\n",
    "                        help='No. word in SEM')\n",
    "    parser.add_argument('--V', type=int, default=10,\n",
    "                        help='word size in SEM')\n",
    "                        \n",
    "        # ---- Head-type\n",
    "    parser.add_argument('--head_type', type=str, default='linear',\n",
    "                        help='Head type in interaction, linear or mlp')    \n",
    "    return parser\n",
    "\n",
    "def rnd_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "rnd_seed(10086)\n",
    "\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(args=[])\n",
    "#args = args.parse_args()\n",
    "args.device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "FIG_DIR = 'E:\\\\P45_disentanglement\\\\figures\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269fe9f",
   "metadata": {},
   "source": [
    "## Prepare for the probing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b58d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name == 'ogbg-molhiv':\n",
    "    smiles_path = 'E:\\\\P4_Graph\\\\dataset\\\\ogbg_molhiv\\\\mapping\\\\mol.csv.gz'\n",
    "    args.batch_size = 4113\n",
    "    args.batch_size_train = 32901\n",
    "elif args.dataset_name == 'ogbg-molpcba':\n",
    "    args.batch_size = 24000\n",
    "    args.batch_size_train = 350343\n",
    "    smiles_path = 'E:\\\\P4_Graph\\\\dataset\\\\ogbg_molpcba\\\\mapping\\\\mol.csv.gz'\n",
    "elif args.dataset_name =='ogbg-moltox21':\n",
    "    args.batch_size = 783\n",
    "    smiles_path = 'E:\\\\P4_Graph\\\\dataset\\\\ogbg_moltox21\\\\mapping\\\\mol.csv.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1902b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling probing.compute_properties...\n",
      "compute_properties([ 'CC(=O)N1CCC2(CC1)NC(=O)N(c1ccccc1)N2',\n",
      "  'N#Cc1nnn(-c2ccc(Cl)cc2)c1N',\n",
      "  'COC(=O)c1ccc(NC(=O)c2ccccc2CC[N+](=O)[O-])cc1',\n",
      "  'CCC1NC(=O)c2cccnc2-n2c1nc1ccc(F)cc1c2=O',\n",
      "  'CCOC(=O)C1=C(C)NC(=O)NC1c1cccs1',\n",
      "  'Cc1sc2ncnc(NCc3ccco3)c2c1C',\n",
      "  'CCOC(=O)C1=C(C)NC(=O)NC1c1ccc(N(C)C)cc1',\n",
      "  'C/N=C(\\\\N)NC(=O)Nc1c(C)cccc1C',\n",
      "  'Cn1ccnc1SCC(=O)Nc1ccc([N+](=O)[O-])cn1',\n",
      "  'COc1ccc(CNC(=O)C2(CC3CC(c4ccccc4)=NO3)CCN(C(=O)OC(C)(C)C)CC2)cc1',\n",
      "  'Cc1ccc(C(=O)N/C(=C\\\\c2cn(-c3ccccc3)nc2-c2ccccc2)C(=O)N2CCOCC2)cc1',\n",
      "  'O=C(N/N=C/c1ccc(OC(=O)c2ccccc2Br)cc1)c1ccccn1',\n",
      "  'Cc1ccc2c(c1)c1c(n2CC(O)CNC(C)(C)C)CCCC1',\n",
      "  'CS(=O)(=O)N(Cc1ccccc1)c1ccc(C(=O)Nc2ccccc2C(=O)O)cc1',\n",
      "  'O=C(CSc1nc2ccccc2s1)N1c2ccccc2Sc2ccc(C(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████████▌                                                                                           | 61741/350343 [11:36<52:13, 92.11it/s][21:50:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:50:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:50:57] WARNING: not removing hydrogen atom without neighbors\n",
      " 27%|█████████████████████████████▍                                                                                 | 92888/350343 [17:26<49:02, 87.50it/s][21:56:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:56:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:56:47] WARNING: not removing hydrogen atom without neighbors\n",
      " 63%|█████████████████████████████████████████████████████████████████████▋                                        | 222041/350343 [41:35<23:39, 90.38it/s][22:20:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:20:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:20:56] WARNING: not removing hydrogen atom without neighbors\n",
      " 71%|██████████████████████████████████████████████████████████████████████████████▌                               | 250249/350343 [46:55<17:25, 95.72it/s][22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:26:16] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 350343/350343 [1:06:27<00:00, 87.85it/s]\n",
      "C:\\Users\\YIREN\\AppData\\Local\\Temp\\ipykernel_10208\\1604928335.py:32: UserWarning: Persisting input arguments took 1.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  train_desc_names, train_properties = compute_properties(train_smiles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________compute_properties - 4008.9s, 66.8min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_prop = ['NumSaturatedRings', 'NumAromaticRings', 'NumAromaticCarbocycles', 'fr_aniline', \n",
    "                 'fr_ketone', 'fr_bicyclic', 'fr_methoxy', 'fr_para_hydroxylation', 'fr_pyridine', 'fr_benzene']\n",
    "\n",
    "dataset = PygGraphPropPredDataset(name = args.dataset_name)\n",
    "args.num_tasks = dataset.num_tasks\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size_train, shuffle=False, drop_last=True,\n",
    "                        num_workers=args.num_workers)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.batch_size, shuffle=False, drop_last=True,\n",
    "                        num_workers=args.num_workers)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, drop_last=True,\n",
    "                        num_workers=args.num_workers)\n",
    "\n",
    "train_smiles = pd.read_csv(smiles_path).iloc[split_idx['train']].smiles.values\n",
    "train_smiles = train_smiles.tolist()\n",
    "valid_smiles = pd.read_csv(smiles_path).iloc[split_idx['valid']].smiles.values\n",
    "valid_smiles = valid_smiles.tolist()\n",
    "test_smiles = pd.read_csv(smiles_path).iloc[split_idx['test']].smiles.values\n",
    "test_smiles = test_smiles.tolist()\n",
    "\n",
    "train_desc_names, train_properties = compute_properties(train_smiles)\n",
    "valid_desc_names, valid_properties = compute_properties(valid_smiles)\n",
    "test_desc_names, test_properties = compute_properties(test_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270810b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1111.25it/s]\n"
     ]
    }
   ],
   "source": [
    "train_fp, valid_fp, test_fp = [], [], []\n",
    "for prop in tqdm(selected_prop):\n",
    "    train_fp.append(np.array(train_properties[prop].values[:args.batch_size_train]>0,dtype=int))\n",
    "    valid_fp.append(np.array(valid_properties[prop].values[:args.batch_size]>0,dtype=int))\n",
    "    test_fp.append(np.array(test_properties[prop].values[:args.batch_size]>0,dtype=int))\n",
    "train_fp = np.array(train_fp).transpose()\n",
    "valid_fp = np.array(valid_fp).transpose()\n",
    "test_fp = np.array(test_fp).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96be8298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 350343/350343 [04:26<00:00, 1314.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----- For each sample in validation set, sweep the training set to see how many samples share similar finger print\n",
    "# -- This will generate a long list of the number of duplicates each training sample have\n",
    "index_template = np.arange(0,args.batch_size,1)\n",
    "dup_count_list, dup_index_list = [],[]\n",
    "for i in tqdm(range(args.batch_size_train)):\n",
    "    anchor = train_fp[i]\n",
    "    tmp1 = valid_fp.dot(anchor)==(anchor.sum())\n",
    "    tmp2 = valid_fp.dot(anchor)==valid_fp.sum(1)\n",
    "    dup_mask = np.logical_and(tmp1,tmp2)\n",
    "    dup_count = dup_mask.sum()\n",
    "    dup_index = index_template[dup_mask]\n",
    "    dup_count_list.append(dup_count)\n",
    "    dup_index_list.append(dup_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8b095f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- The dup_count_list stores how many similar samples are in the validation set\n",
    "# --- To make the problem more challenging, we need to first argsort this list, and delete those numbers with the HIGHER overlapping FP's\n",
    "# --- After that, there might be less similar samples in the training set\n",
    "PRUNE_IDX = int(args.batch_size_train*0.95)   # How many data samples are pruned\n",
    "train_index_template = np.arange(0,args.batch_size_train,1)\n",
    "train_split_index = np.array(split_idx['train'])\n",
    "dup_count_list = np.array(dup_count_list)\n",
    "tmp_pd = pd.DataFrame(np.column_stack((train_index_template, dup_count_list,train_split_index)),\n",
    "                      index=train_index_template, columns=['index','count','split_index'])\n",
    "prune_pd = tmp_pd.sort_values(by='count',ascending=False)[PRUNE_IDX:]\n",
    "prune_pd_sort = prune_pd.sort_values(by='index',ascending=True)\n",
    "\n",
    "# ----- The index of the pruned dataset is saved in this npy file, during training, we use it to select samples\n",
    "sel_index = prune_pd_sort.iloc[:,0]\n",
    "sel_index = sel_index.values\n",
    "save_path = 'E:\\\\P4_Graph\\\\dataset\\\\' + args.dataset_name + '_hard.npy'\n",
    "np.save(save_path, sel_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bee4937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3504,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "288478b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_pd_sort['count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec7b7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_pd = tmp_pd.sort_values(by='count',ascending=False)\n",
    "prune_pd_sort = prune_pd.sort_values(by='index',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22670406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_pd['count'][-1000:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb6cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
